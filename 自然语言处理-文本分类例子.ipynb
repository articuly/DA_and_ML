{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Natura Language Tool Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'coolest', 'job', 'in', 'the', 'next', '10', 'years', 'will', 'be', 'statisticians', '.', 'People', 'think', 'I', \"'m\", 'joking', ',', 'but', 'who', 'would', \"'ve\", 'guessed', 'that', 'computer', 'engineers', 'would', \"'ve\", 'been', 'the', 'cooleast', 'job', 'of', 'the', '1990s', '?']\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "my_text = \"The coolest job in the next 10 years will be statisticians. \\\n",
    "           People think I'm joking, but who would've guessed that \\\n",
    "           computer engineers would've been the cooleast job of the 1990s?\"\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(my_text)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':', 'I', 'looove', 'this', 'city', '!', '!', '!', '#love', '#foreverhere']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet分词\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet = '@mate: I looooooove this city!!!!!!! #love #foreverhere'\n",
    "tt.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'coolest', 'job', 'in', 'the', 'next', '10', 'year', 'wil', 'be', 'stat', '.', 'peopl', 'think', 'i', \"'m\", 'jok', ',', 'but', 'who', 'would', \"'ve\", 'guess', 'that', 'comput', 'engin', 'would', \"'ve\", 'been', 'the', 'cooleast', 'job', 'of', 'the', '1990s', '?']\n"
     ]
    }
   ],
   "source": [
    "# 词干提取\n",
    "from nltk.stem import *\n",
    "stemmer = LancasterStemmer()\n",
    "print([stemmer.stem(word) for word in nltk_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('coolest', 'JJS'), ('job', 'NN'), ('in', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('10', 'CD'), ('years', 'NNS'), ('will', 'MD'), ('be', 'VB'), ('statisticians', 'NNS'), ('.', '.'), ('People', 'NNS'), ('think', 'VBP'), ('I', 'PRP'), (\"'m\", 'VBP'), ('joking', 'VBG'), (',', ','), ('but', 'CC'), ('who', 'WP'), ('would', 'MD'), (\"'ve\", 'VBP'), ('guessed', 'VBN'), ('that', 'IN'), ('computer', 'NN'), ('engineers', 'NNS'), ('would', 'MD'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('the', 'DT'), ('cooleast', 'JJ'), ('job', 'NN'), ('of', 'IN'), ('the', 'DT'), ('1990s', 'CD'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 词性标注\n",
    "print(nltk.pos_tag(nltk_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 命名实体识别\n",
    "text = \"Elvis Aaron Presley was an American singer and actor. Born in \\\n",
    "      Tupelo, Mississippi, when Presley was 13 years old he and his \\\n",
    "      family relocated to Memphis, Tennessee.\"\n",
    "\n",
    "chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elvis', 'NNP'),\n",
       " ('Aaron', 'NNP'),\n",
       " ('Presley', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('an', 'DT'),\n",
       " ('American', 'JJ'),\n",
       " ('singer', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('actor', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Born', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Tupelo', 'NNP'),\n",
       " (',', ','),\n",
       " ('Mississippi', 'NNP'),\n",
       " (',', ','),\n",
       " ('when', 'WRB'),\n",
       " ('Presley', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('13', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " ('he', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('his', 'PRP$'),\n",
       " ('family', 'NN'),\n",
       " ('relocated', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('Memphis', 'NNP'),\n",
       " (',', ','),\n",
       " ('Tennessee', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Elvis/NNP)\n",
      "  (PERSON Aaron/NNP Presley/NNP)\n",
      "  was/VBD\n",
      "  an/DT\n",
      "  (GPE American/JJ)\n",
      "  singer/NN\n",
      "  and/CC\n",
      "  actor/NN\n",
      "  ./.\n",
      "  Born/VBN\n",
      "  in/IN\n",
      "  (GPE Tupelo/NNP)\n",
      "  ,/,\n",
      "  (GPE Mississippi/NNP)\n",
      "  ,/,\n",
      "  when/WRB\n",
      "  (PERSON Presley/NNP)\n",
      "  was/VBD\n",
      "  13/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  he/PRP\n",
      "  and/CC\n",
      "  his/PRP$\n",
      "  family/NN\n",
      "  relocated/VBD\n",
      "  to/TO\n",
      "  (GPE Memphis/NNP)\n",
      "  ,/,\n",
      "  (GPE Tennessee/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'call', 'may', 'bill', 'get', 'latter', 'off', 'done', 'name', 'per', 'whole', 'yours', 'one', 'from', 're', 'fire', 'co', 'hundred', 'others', 'where', 'an', 'being', 'already', 'together', 'us', 'if', 'might', 'toward', 'until', 'around', 'there', 'behind', 'both', 'find', 'after', 'and', 'out', 'eleven', 'however', 'so', 'except', 'but', 'he', 'side', 'its', 'when', 'last', 'while', 'besides', 'also', 'because', 'sixty', 'eg', 'fifteen', 'another', 'etc', 'why', 'something', 'those', 'interest', 'six', 'latterly', 'ever', 'hers', 'anything', 'front', 'before', 'i', 'seem', 'their', 'anyone', 'mostly', 'made', 'ten', 'will', 'hereby', 'thence', 'empty', 'two', 'whither', 'than', 'has', 'me', 'to', 'found', 'forty', 'throughout', 'beforehand', 'been', 'un', 'her', 'moreover', 'becomes', 'former', 'here', 'hereafter', 'often', 'describe', 'whoever', 'under', 'formerly', 'take', 'themselves', 'top', 'amount', 'thick', 'with', 'thin', 'as', 'by', 'how', 'less', 'own', 'otherwise', 'thru', 'over', 'nor', 'perhaps', 'against', 'these', 'again', 'rather', 'move', 'him', 'well', 'therefore', 'only', 'down', 'please', 'serious', 'ltd', 'wherever', 'few', 'you', 'our', 'on', 'first', 'beyond', 'least', 'via', 'yourselves', 'several', 'detail', 'the', 'everywhere', 'himself', 'not', 'it', 'along', 'had', 'even', 'become', 'every', 'this', 'hereupon', 'more', 'in', 'seeming', 'can', 'most', 'itself', 'must', 'ie', 'anyhow', 'becoming', 'show', 'much', 'thereby', 'yet', 'de', 'other', 'during', 'nobody', 'enough', 'is', 'are', 'whatever', 'bottom', 'amoungst', 'for', 'that', 'herself', 'third', 'amongst', 'sometimes', 'should', 'sometime', 'they', 'namely', 'back', 'your', 'would', 'myself', 'go', 'became', 'many', 'no', 'never', 'beside', 'alone', 'therein', 'same', 'hasnt', 'them', 'fill', 'whence', 'see', 'still', 'afterwards', 'were', 'give', 'indeed', 'nowhere', 'all', 'elsewhere', 'am', 'twelve', 'upon', 'hence', 'next', 'through', 'somehow', 'among', 'con', 'everything', 'whose', 'about', 'towards', 'eight', 'always', 'cannot', 'a', 'thereupon', 'almost', 'was', 'have', 'onto', 'fifty', 'further', 'thus', 'herein', 'who', 'due', 'below', 'couldnt', 'very', 'neither', 'each', 'cry', 'noone', 'put', 'twenty', 'do', 'everyone', 'up', 'she', 'someone', 'too', 'whereas', 'since', 'ourselves', 'else', 'seems', 'five', 'either', 'part', 'some', 'keep', 'we', 'cant', 'between', 'be', 'whereafter', 'full', 'or', 'whereupon', 'anyway', 'such', 'his', 'nine', 'nevertheless', 'any', 'could', 'whom', 'somewhere', 'once', 'anywhere', 'my', 'now', 'into', 'which', 'four', 'what', 'none', 'above', 'whereby', 'inc', 'ours', 'seemed', 'whether', 'mill', 'mine', 'sincere', 'system', 'yourself', 'whenever', 'within', 'meanwhile', 'thereafter', 'although', 'at', 'then', 'without', 'across', 'three', 'nothing', 'though', 'of', 'wherein'})\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass', 'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本分类例子\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "cat = ['sci.med', 'sci.space']\n",
    "to_remove = ('headers', 'footers', 'quotes')\n",
    "news_train = fetch_20newsgroups(subset='train',\n",
    "                                remove=to_remove,\n",
    "                                categories=cat)\n",
    "news_test = fetch_20newsgroups(subset='test', remove=to_remove, categories=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(news_train.data)\n",
    "X_test = tfidf.transform(news_test.data)\n",
    "y_train = news_train.target\n",
    "y_test = news_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8911392405063291\n",
      "Wall time: 56.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SGDClassifier(fit_intercept=False, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy = {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_stem_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    clean_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stem_tokens = [stemmer.stem(tokens) for tokens in clean_tokens]\n",
    "    return ' '.join(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_docs_train = [clean_and_stem_text(text) for text in news_train.data]\n",
    "cleaned_docs_test = [clean_and_stem_text(text) for text in news_test.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8936708860759494\n"
     ]
    }
   ],
   "source": [
    "X1_train = tfidf.fit_transform(cleaned_docs_train)\n",
    "X1_text = tfidf.transform(cleaned_docs_test)\n",
    "clf.fit(X1_train, y_train)\n",
    "y1_pred = clf.predict(X1_text)\n",
    "print('Accuracy = {}'.format(accuracy_score(y_test, y1_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隐含狄利克雷分布 Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return [\n",
    "        token.lower()  # 转小写\n",
    "        for token in gensim.utils.simple_preprocess(\n",
    "            text)  # 分词，过滤小于2个字母大于15个字母的单词\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS  # 去掉停用词\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gensim.parsing.preprocessing.STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1752\n"
     ]
    }
   ],
   "source": [
    "text_data = fetch_20newsgroups(\n",
    "    categories=['rec.autos', 'sci.med', 'talk.politics.mideast'],\n",
    "    random_state=101,\n",
    "    remove=to_remove)\n",
    "docs = text_data.data\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gee, what do I do?  My LDL is only 50-60. (and my HDL is only 23-25)\n",
      "I must be risking something, but Is it the same risk as those with \n",
      "very high LDL?\n",
      "\n",
      "\n",
      "What about exercise and a low-fat diet?  What are the long-term \n",
      "effects of this drug?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back when I was building round tail light 2002s they were Bimmers.  It was\n",
      "only when the (red suspendered, Reganomics generated, quiche eating) Yuppies\n",
      "got into the market >-( that they became Beamers and the hood ornaments started\n",
      "disappering.\n"
     ]
    }
   ],
   "source": [
    "print(docs[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = [tokenize(doc) for doc in docs]\n",
    "word_dic = gensim.corpora.Dictionary(processed_docs)  # 类似词袋字典对象，有多种属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1752\n",
      "177568\n",
      "122067\n"
     ]
    }
   ],
   "source": [
    "print(word_dic.num_docs)\n",
    "print(word_dic.num_pos)\n",
    "print(word_dic.num_nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23901"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dic)  # 包括不同的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dic.filter_extremes(no_below=10, no_above=0.2)  # 保留至少出现10次的词，且不超过20%\n",
    "bow = [word_dic.doc2bow(doc) for doc in processed_docs]  # 生成1752个文档的词袋列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1752"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(\n",
    "    bow,  # 词袋列表\n",
    "    num_topics=3,  # 分类主题数\n",
    "    id2word=word_dic,  # Dictionary对象\n",
    "    passes=16,  #  通道\n",
    "    iterations=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"armenian\" + 0.011*\"armenians\" + 0.011*\"said\" + 0.010*\"turkish\" + 0.006*\"jews\" + 0.006*\"israel\" + 0.005*\"turkey\" + 0.005*\"israeli\" + 0.005*\"went\" + 0.005*\"government\"'),\n",
       " (1,\n",
       "  '0.012*\"edu\" + 0.009*\"com\" + 0.008*\"health\" + 0.007*\"medical\" + 0.007*\"university\" + 0.007*\"use\" + 0.006*\"new\" + 0.006*\"information\" + 0.005*\"research\" + 0.005*\"number\"'),\n",
       " (2,\n",
       "  '0.012*\"car\" + 0.008*\"think\" + 0.006*\"good\" + 0.006*\"time\" + 0.005*\"israel\" + 0.005*\"right\" + 0.004*\"problem\" + 0.004*\"cars\" + 0.004*\"new\" + 0.004*\"ve\"')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印所有主题\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"I've shown the doctor my new car. He loved its big wheels!\"\n",
    "bow_doc = word_dic.doc2bow(tokenize(new_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(167, 1), (487, 1), (491, 1), (551, 1), (559, 1), (1627, 1), (2173, 1)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9090130925178528, Topic: 0.012*\"car\" + 0.008*\"think\" + 0.006*\"good\" + 0.006*\"time\" + 0.005*\"israel\"\n",
      "Score: 0.046745315194129944, Topic: 0.012*\"edu\" + 0.009*\"com\" + 0.008*\"health\" + 0.007*\"medical\" + 0.007*\"university\"\n",
      "Score: 0.04424162209033966, Topic: 0.013*\"armenian\" + 0.011*\"armenians\" + 0.011*\"said\" + 0.010*\"turkish\" + 0.006*\"jews\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_doc],\n",
    "                           key=lambda tup: tup[1],\n",
    "                           reverse=True):\n",
    "    print('Score: {}, Topic: {}'.format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.044241548), (1, 0.046725716), (2, 0.90903276)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model[bow_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.9090123), (1, 0.04674605), (0, 0.044241637)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lda_model[bow_doc], key=lambda tup: -1 * tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.012*\"car\" + 0.008*\"think\" + 0.006*\"good\" + 0.006*\"time\" + 0.005*\"israel\"'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topic(2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import movie_reviews\n",
    "w2v = Word2Vec(movie_reviews.sents(), workers=4)\n",
    "w2v.init_sims(replace=True)  # 固定模型，不再更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wedding', 0.8668364882469177),\n",
       " ('hotel', 0.8647732138633728),\n",
       " ('apartment', 0.8610095977783203),\n",
       " ('country', 0.856749415397644),\n",
       " ('body', 0.8553558588027954),\n",
       " ('station', 0.8445186614990234),\n",
       " ('local', 0.8440505266189575),\n",
       " ('party', 0.8419867753982544),\n",
       " ('bar', 0.8363257050514221),\n",
       " ('living', 0.8347684741020203)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('house', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paralyzed', 0.9465136528015137),\n",
       " ('causing', 0.9431409239768982),\n",
       " ('border', 0.9419759511947632),\n",
       " ('online', 0.9393081068992615),\n",
       " ('farmers', 0.9381197690963745),\n",
       " ('internal', 0.9377422332763672),\n",
       " ('wound', 0.9375422596931458),\n",
       " ('mining', 0.9375393986701965),\n",
       " ('union', 0.937467634677887),\n",
       " ('rig', 0.9372416734695435)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('countryside', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02427886,  0.09660256, -0.00637537, -0.15946592,  0.14721759,\n",
       "       -0.1368917 , -0.12395467, -0.16306636, -0.02574619, -0.00405329,\n",
       "        0.05253716, -0.05105058,  0.07331239,  0.04443988,  0.06931428,\n",
       "       -0.04729541,  0.00521677,  0.0782991 ,  0.1307938 , -0.16089483,\n",
       "       -0.06671596,  0.06017288, -0.08920097,  0.08610193,  0.05218294,\n",
       "        0.0019553 ,  0.03128374, -0.02785585, -0.16492818,  0.15375887,\n",
       "       -0.10946473,  0.14962548,  0.06548597, -0.00377436, -0.04982037,\n",
       "        0.03007328,  0.18657072,  0.14839248,  0.02354456, -0.07350426,\n",
       "        0.03854439, -0.02674351,  0.01844085, -0.00599654, -0.00094211,\n",
       "        0.00873501,  0.00364529,  0.08921657,  0.09013582,  0.08390693,\n",
       "        0.1057057 ,  0.10874041,  0.02966209, -0.02938473,  0.06717335,\n",
       "       -0.06369507,  0.0848505 ,  0.09073838, -0.10945512, -0.03622925,\n",
       "       -0.16745006, -0.03348833, -0.06317479,  0.1475254 ,  0.09494581,\n",
       "       -0.17496276, -0.10336761,  0.24626589, -0.10652564, -0.0318913 ,\n",
       "        0.10719876, -0.15205172, -0.08157685,  0.01818673, -0.17415106,\n",
       "        0.06753524,  0.02845405, -0.0498521 ,  0.10635109, -0.15752912,\n",
       "        0.05938824, -0.12631789, -0.01104039,  0.02814195,  0.09319299,\n",
       "        0.00049126, -0.09984824,  0.21064577, -0.10420449, -0.11264102,\n",
       "       -0.02367065, -0.07844702, -0.33450738,  0.08576859,  0.017945  ,\n",
       "        0.02837355, -0.01405297, -0.03384067,  0.08057533, -0.0718817 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['house']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.48700371e-02,  7.00271726e-02, -7.67491460e-02, -6.84995055e-02,\n",
       "        9.55135524e-02, -1.18068002e-01, -1.72842339e-01, -1.64749339e-01,\n",
       "       -5.28507009e-02, -9.40757245e-03, -4.23805369e-03, -7.75362998e-02,\n",
       "        1.07264578e-01,  4.99418564e-02,  5.68794012e-02, -6.74575195e-02,\n",
       "        8.57803375e-02,  1.17778189e-01,  1.87019721e-01, -1.21144824e-01,\n",
       "       -1.65744275e-01,  5.42791784e-02, -4.14649732e-02,  6.22955151e-02,\n",
       "        6.93899244e-02, -4.25398797e-02, -8.10289010e-02, -5.18517531e-02,\n",
       "       -1.30884618e-01,  5.47503196e-02, -1.29633218e-01,  9.46059078e-02,\n",
       "        5.73539883e-02,  1.78467035e-02, -8.18712637e-02,  1.13160320e-01,\n",
       "        1.77643284e-01,  1.31120637e-01,  5.33079170e-02, -3.50715742e-02,\n",
       "        2.15164050e-02,  2.21600458e-02, -5.59382699e-02,  4.99225147e-02,\n",
       "       -4.78981389e-03, -3.33971158e-02,  2.51089353e-02, -5.58985211e-02,\n",
       "        9.57650691e-02,  9.81157273e-02,  1.17004447e-01,  1.09301731e-01,\n",
       "        2.22818702e-02,  4.16154899e-02,  7.73291960e-02, -1.06022224e-01,\n",
       "        2.03302838e-02,  6.46115765e-02, -1.39863372e-01,  1.17965005e-02,\n",
       "       -2.08333343e-01,  9.47148576e-02, -1.24619402e-01,  2.08204389e-01,\n",
       "        9.51578394e-02, -2.28801489e-01, -1.12716496e-01,  1.80318117e-01,\n",
       "       -1.82110652e-01, -5.70060313e-02,  1.17820635e-01, -1.13191217e-01,\n",
       "       -6.86621666e-02, -3.54588008e-03, -1.05576672e-01,  5.91263101e-02,\n",
       "        7.92586133e-02, -5.18514402e-02,  1.45326719e-01, -2.55955905e-01,\n",
       "        1.03140347e-01, -1.34153143e-01,  1.75684094e-04,  8.86391401e-02,\n",
       "        5.68200722e-02, -9.19177104e-03, -1.40053444e-02,  6.51820451e-02,\n",
       "       -1.00913987e-01, -7.76552558e-02,  2.72204746e-02, -5.10647483e-02,\n",
       "       -2.38565505e-01,  3.95293981e-02, -7.72476615e-03,  7.23551288e-02,\n",
       "       -3.93840065e-03,  3.44762579e-02,  2.07292270e-02, -1.05811423e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['hotel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['hotel'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lady', 0.8865196704864502),\n",
       " ('partner', 0.8856627941131592),\n",
       " ('former', 0.8688048124313354),\n",
       " ('daughter', 0.8662325143814087),\n",
       " ('jack', 0.8644030094146729)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=['woman', 'king'], topn=5)  # 多正例相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.821386992931366),\n",
       " ('boy', 0.7598280906677246),\n",
       " ('child', 0.7559627294540405),\n",
       " ('father', 0.7236120700836182),\n",
       " ('girl', 0.7173840999603271)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=['woman', 'king'], negative=['queen'],\n",
    "                    topn=5)  # 正例相似，加负例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lady', 0.9145256280899048),\n",
       " ('lawyer', 0.8811653852462769),\n",
       " ('former', 0.878139853477478),\n",
       " ('partner', 0.87492436170578),\n",
       " ('girl', 0.8732819557189941)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=['woman', 'queen'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programing\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cake'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.doesnt_match(['bed', 'pillow', 'cake', 'mattress'])  # 不匹配的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9102032, 0.8507879)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity('woman', 'girl'), w2v.similarity('woman', 'boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
